{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1번. CNN & RNN\n",
    "\n",
    "## 1.1 CNN cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "def seed_everything(seed):\n",
    "    #random.seed(seed)\n",
    "    #os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "seed_everything(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "# prepare Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "batch_size = 8\n",
    "valid_data_size = 10000\n",
    "trainset = torchvision.datasets.CIFAR10(root='./cifar10_data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train, validset = torch.utils.data.random_split(trainset, [len(trainset) - valid_data_size, valid_data_size])\n",
    "testset = torchvision.datasets.CIFAR10(root='./cifar10_data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "print(len(trainloader.dataset),len(validloader.dataset),len(testloader.dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainig code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(model_name,model,train_loader,valid_loader,device,save_name='best'):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01,momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                            lr_lambda=lambda epoch: 1.0 if epoch<=5 else 0.94**(epoch-5))\n",
    "    n_epochs = 100\n",
    "\n",
    "    train_loss_list , valid_loss_list = [] , []\n",
    "    best_score = float('inf')\n",
    "\n",
    "    model.train()\n",
    "    patience = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            data , target = data.to(device) , target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "        scheduler.step()\n",
    "        train_loss_list.append(train_loss/len(train_loader.dataset))\n",
    "\n",
    "        # valid_data\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0.0\n",
    "            correct_num = 0.0\n",
    "\n",
    "            for data, target in valid_loader:\n",
    "                data , target = data.to(device) , target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "                predicted_classes = torch.argmax(output, dim=1)\n",
    "                correct_num += (predicted_classes == target).sum().item()\n",
    "            valid_loss_list.append(valid_loss/len(valid_loader.dataset))\n",
    "            correct_num /= len(valid_loader.dataset)\n",
    "        \n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\t valid Loss: {:.6f} \\t valid Acc{}'.format(epoch+1, train_loss_list[-1],valid_loss_list[-1],correct_num))\n",
    "            \n",
    "        if valid_loss_list[-1] < best_score:\n",
    "            best_score = valid_loss_list[-1]\n",
    "            torch.save(model.state_dict(), os.path.join('./model_data/', '{}_{}.pth'.format(model_name,save_name)))\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience == 8:\n",
    "                return train_loss_list , valid_loss_list\n",
    "\n",
    "def test_model(model_name,model,test_loader,device):\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "        for i in range(len(target.data)):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "        \n",
    "            \n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    print(f'{model_name}=========================================================')\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "                str(i), 100 * class_correct[i] / class_total[i],\n",
    "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "        else:\n",
    "            print('Test Accuracy of %5s: N/A (no training examples)' % (str(i)))\n",
    "\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "        100. * np.sum(class_correct) / np.sum(class_total),\n",
    "        np.sum(class_correct), np.sum(class_total)))\n",
    "    print('==========================================================================')\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet/VGG/ResNet/DenseNet 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    [common preprocessing]\n",
    "        resie 256*256 -> central_crop 224*224  \n",
    "        mean=[0.485, 0.456, 0.406] std=[0.229, 0.224, 0.225]\n",
    "\n",
    "    [AlexNet config]\n",
    "        num_params : 61,100,840\n",
    "\n",
    "    [VGG19]\n",
    "        BN은 사용x 후에 Batch Normalization에서 적용\n",
    "        num_params : 143,667,240\n",
    "\n",
    "    [ResNet50]\n",
    "        50 vs 101 vs 152가 성능이 비슷비슷해서 효율이 좋은 ResNet50 사용\n",
    "        num_params : 25,557,032\n",
    "    \n",
    "    [DenseNet161]\n",
    "        121부터 201까지 있는데 가장 성능이 좋은 161사용 \n",
    "        특이하게도 layer수가 많아질수록 imageNet 성능이 안좋음\n",
    "        num_params : 28,681,000\n",
    "'''\n",
    "\n",
    "from torchvision.models import alexnet, AlexNet_Weights \n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models import densenet161, DenseNet161_Weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model finetuing\n",
    "\n",
    "기존의 pretrained 모델은 224*224 image입력을 받음\n",
    "\n",
    "cifar10은 32*32 image임\n",
    "\n",
    "따라서 max pooling 모두 제거 , adaptivepooling으로 최종 feature size맞추기\n",
    "\n",
    "ResNet과 DenseNet은 첫번째 pooling만 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetuned_model(model_name,update_conv=False):\n",
    "    if model_name == 'alex' : \n",
    "        model = alexnet(weights=AlexNet_Weights)\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = update_conv\n",
    "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
    "        return model\n",
    "\n",
    "    elif model_name == 'vgg' : \n",
    "        model = vgg19(weights=VGG19_Weights)\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = update_conv\n",
    "        model.classifier[6] = nn.Linear(4096, 10)\n",
    "        return model\n",
    "        \n",
    "\n",
    "    elif model_name == 'resnet':\n",
    "        model = resnet50(weights=ResNet50_Weights)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = update_conv\n",
    "        model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "        return model\n",
    "    \n",
    "    elif model_name == 'densenet':\n",
    "        model = densenet161(weights=DenseNet161_Weights)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = update_conv\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, 10)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "densenet training start\n",
      "Epoch: 1 \tTraining Loss: 1.820886 \t valid Loss: 1.356005 \t valid Acc0.5232\n",
      "Epoch: 2 \tTraining Loss: 1.184854 \t valid Loss: 1.021823 \t valid Acc0.6416\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m training start\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m training_dic[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m] , training_dic[\u001b[39m'\u001b[39m\u001b[39mvalid_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m training_model(model_name,model,trainloader,validloader,device,save_name\u001b[39m=\u001b[39;49msave_name)\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m training end\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[39mdel\u001b[39;00m model\n",
      "Cell \u001b[1;32mIn[33], line 23\u001b[0m, in \u001b[0;36mtraining_model\u001b[1;34m(model_name, model, train_loader, valid_loader, device, save_name)\u001b[0m\n\u001b[0;32m     21\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     22\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> 23\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\u001b[39m*\u001b[39mdata\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     24\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m train_loss_list\u001b[39m.\u001b[39mappend(train_loss\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(train_loader\u001b[39m.\u001b[39mdataset))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "#model_list = ['alex','vgg','resnet','densenet']\n",
    "model_list = ['densenet']\n",
    "acc_dic = dict()\n",
    "update_conv_token = True\n",
    "save_name = 'best_true' if update_conv_token else 'best_false'\n",
    "\n",
    "for model_name in model_list:\n",
    "    training_dic = {\n",
    "        'train_loss' : [] , 'valid_loss' : []\n",
    "    }\n",
    "\n",
    "    model = load_finetuned_model(model_name,update_conv=update_conv_token)\n",
    "    #model.load_state_dict(torch.load(os.path.join('./model_data/', '{}_{}.pth'.format(model_name,save_name))))\n",
    "    model.to(device)\n",
    "    print(f'{model_name} training start')\n",
    "    training_dic['train_loss'] , training_dic['valid_loss'] = training_model(model_name,model,trainloader,validloader,device,save_name=save_name)\n",
    "    print(f'{model_name} training end')\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alex=========================================================\n",
      "Test Loss: 0.586114\n",
      "\n",
      "Test Accuracy of     0: 82% (820/1000)\n",
      "Test Accuracy of     1: 80% (809/1000)\n",
      "Test Accuracy of     2: 73% (733/1000)\n",
      "Test Accuracy of     3: 68% (683/1000)\n",
      "Test Accuracy of     4: 63% (639/1000)\n",
      "Test Accuracy of     5: 83% (834/1000)\n",
      "Test Accuracy of     6: 87% (871/1000)\n",
      "Test Accuracy of     7: 72% (729/1000)\n",
      "Test Accuracy of     8: 88% (888/1000)\n",
      "Test Accuracy of     9: 93% (931/1000)\n",
      "\n",
      "Test Accuracy (Overall): 79% (7937/10000)\n",
      "==========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "model_list = ['resnet','densenet']\n",
    "update_conv_token = True\n",
    "save_name = 'best_true' if update_conv_token else 'best_false'\n",
    "\n",
    "for model_name in model_list:\n",
    "    model = load_finetuned_model(model_name)\n",
    "    model.load_state_dict(torch.load(os.path.join('./model_data/', '{}_{}.pth'.format(model_name,save_name))))\n",
    "    model.to(device)\n",
    "    test_model(model_name,model,testloader,device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||test_acc|error_rate|train_loss|valid_loss|epoch|\n",
    "|------|---|---|---|---|---|\n",
    "|AlexNet|%|%||||\n",
    "|AlexNet with conv train|92.37%|7.63%|0.0021|0.0014||\n",
    "|VGG19|%|%||||\n",
    "|VGG19 with conv train|87.37%|12.63%|0.0007|0.0005||\n",
    "|ResNet50|%|%||||\n",
    "|ResNet50 with conv train|%|%||||\n",
    "|DenseNet161|%|%||||\n",
    "|DenseNet161 with conv train|%|%||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
