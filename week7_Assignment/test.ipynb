{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 음식배달에 걸리는 시간 예측하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time 2 vec : https://towardsdatascience.com/time2vec-for-time-series-features-encoding-a03a4f3f937e\n",
    "\n",
    "https://ojus1.github.io/posts/time2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============결측치 개수=============\n",
      "총 데이터 개수 : 197428개\n",
      "market_id : 987\n",
      "actual_delivery_time : 7\n",
      "store_primary_category : 4760\n",
      "order_protocol : 995\n",
      "total_onshift : 16262\n",
      "total_busy : 16262\n",
      "total_outstanding_orders : 16262\n",
      "estimated_store_to_consumer_driving_duration : 526\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./delivery_raw.csv',sep='\\t')\n",
    "print('=============결측치 개수=============')\n",
    "print(f'총 데이터 개수 : {len(data)}개')\n",
    "for key in data.keys():\n",
    "    num = data[key].isna().sum()\n",
    "    if num: print(f'{key} : {num}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결측치 처리\n",
    "- market_id : 지역번호로 987개밖에 없으므로 삭제 or 랜덤\n",
    "- actual_delivery_time : 예측해야하는 값이므로 dropna\n",
    "- store_primary_category : 음식종류로 unique값은 74개이며 결측치 4760개가 아깝긴 하지만 drop?\n",
    "- order_protocol : 주문방법으로 unique값은 7개 , 995개의 결측치 drop?\n",
    "- total_onshift,busy,outstanding_orders : 결측치가 엄청 많음 drop하긴 아까움 -> ?????????????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21651개의 null data 삭제\n"
     ]
    }
   ],
   "source": [
    "# 결측치 제거\n",
    "\n",
    "data_lenth = len(data)\n",
    "#data.dropna(axis=0, how='any', subset=['actual_delivery_time','market_id','order_protocol','store_primary_category'], inplace=True) \n",
    "data.dropna(axis=0, how='any', subset=data.columns, inplace=True)      # 도착시간이 null인경우 해당 raw삭제\n",
    "data_lenth -= len(data)\n",
    "print(f'{data_lenth}개의 null data 삭제')\n",
    "\n",
    "#data.dropna(axis=0, how='any', subset=['market_id'], inplace=True)   \n",
    "#data['market_id'].fillna(value=np.random.randint(1,7), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 datetime 변환 및 target 만들기\n",
    "data['created_at'] = pd.to_datetime(data['created_at'])\n",
    "data['actual_delivery_time'] = pd.to_datetime(data['actual_delivery_time'])\n",
    "data['target'] = (data['actual_delivery_time'] - data['created_at']).dt.seconds\n",
    "data = data[data['target']<10000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주문날짜는 2015-01~02\n",
    "## 범주형\n",
    "1. 명목형 : market_id, store_id, store_primary_category, order_protocol , 요일\n",
    "2. 순서형\n",
    "\n",
    "## 수치형\n",
    "1. 이산형 : total_items, subtotal, num_distinct_items, min_item_price, max_item_price, total_onshift, total_busy, total_outstanding_orders\n",
    "2. 연속형 : created_at, estimated_store_to_consumer_driving_duration, estimated_order_place_duration , 주문 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 175606 entries, 0 to 197427\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                        Non-Null Count   Dtype         \n",
      "---  ------                                        --------------   -----         \n",
      " 0   market_id                                     175606 non-null  float64       \n",
      " 1   created_at                                    175606 non-null  datetime64[ns]\n",
      " 2   actual_delivery_time                          175606 non-null  datetime64[ns]\n",
      " 3   store_id                                      175606 non-null  int64         \n",
      " 4   store_primary_category                        175606 non-null  object        \n",
      " 5   order_protocol                                175606 non-null  float64       \n",
      " 6   total_items                                   175606 non-null  int64         \n",
      " 7   subtotal                                      175606 non-null  int64         \n",
      " 8   num_distinct_items                            175606 non-null  int64         \n",
      " 9   min_item_price                                175606 non-null  int64         \n",
      " 10  max_item_price                                175606 non-null  int64         \n",
      " 11  total_onshift                                 175606 non-null  float64       \n",
      " 12  total_busy                                    175606 non-null  float64       \n",
      " 13  total_outstanding_orders                      175606 non-null  float64       \n",
      " 14  estimated_order_place_duration                175606 non-null  int64         \n",
      " 15  estimated_store_to_consumer_driving_duration  175606 non-null  float64       \n",
      " 16  target                                        175606 non-null  int64         \n",
      "dtypes: datetime64[ns](2), float64(6), int64(8), object(1)\n",
      "memory usage: 24.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "- 날짜 , 범주형 데이터 , 수치형 데이터로 나눔\n",
    "    - 범주형 데이터에 날짜도 포함시키기\n",
    "\n",
    "    \n",
    "날짜 -> 년 월 일 시간으로 나누기\n",
    "임베딩을 위해선 범주화 해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hour, minute 추출 후 주문시간 만들기 + 요일 만들기\n",
    "\n",
    "data['hour'] = data['created_at'].apply(lambda x: x.hour).astype('category')            # 9~13이 없음\n",
    "data['minute'] = data['created_at'].apply(lambda x: x.minute).astype('category')        # 0~59 모두 있음\n",
    "data['order_time'] = data['created_at'].apply(lambda x: (60*x.hour + x.minute) if x.hour>10 else (60*(x.hour+24) + x.minute))\n",
    "data['day_of_week'] = data['created_at'].apply(lambda x: x.day_name()).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수치형 데이터 : (175606, 11)\n",
      "범주형 데이터 : (175606, 5)\n",
      "(175606, 16) (175606, 1)\n",
      "[(6, 3), (5644, 100), (73, 37), (7, 4), (7, 4)]\n"
     ]
    }
   ],
   "source": [
    "# 수치형 변수\n",
    "numeric_features = ['total_items','subtotal','num_distinct_items','min_item_price','max_item_price','total_onshift','total_busy',\n",
    "                    'total_outstanding_orders','estimated_order_place_duration','estimated_store_to_consumer_driving_duration','order_time']\n",
    "\n",
    "X_data_numeric = np.stack([data[col] for col in numeric_features],1)\n",
    "print(f'수치형 데이터 : {X_data_numeric.shape}')\n",
    "\n",
    "# 범주형 변수\n",
    "categorical_features = ['market_id','store_id','store_primary_category','order_protocol','day_of_week']\n",
    "day_of_week_dic = {'Monday': 0,'Tuesday': 1,'Wednesday': 2,'Thursday': 3,'Friday': 4,'Saturday': 5,'Sunday': 6}\n",
    "store_id_dic = {}\n",
    "for i,v in enumerate(sorted(data['store_id'].unique())):\n",
    "    store_id_dic[v] = i\n",
    "\n",
    "def convert_to_int_day(day):                # 요일\n",
    "    return day_of_week_dic[day]\n",
    "def convert_to_int_category(category):      # 음식 카테고리\n",
    "    return pd.factorize(category)[0]\n",
    "def convert_to_int_store_id(store_id):\n",
    "    return store_id_dic[store_id]\n",
    "\n",
    "data['market_id'] = data['market_id'] - 1                                                   # market_id\n",
    "data['store_id'] = data['store_id'].apply(convert_to_int_store_id)                          # store_id\n",
    "data['store_primary_category'] = convert_to_int_category(data['store_primary_category'])    # store_primary_category\n",
    "data['order_protocol'] = data['order_protocol'] - 1                                         # order_protocol\n",
    "data['day_of_week'] = data['day_of_week'].apply(convert_to_int_day)                         # 요일\n",
    "X_data_categorical = np.stack([data[col] for col in categorical_features],1)\n",
    "print(f'범주형 데이터 : {X_data_categorical.shape}')\n",
    "\n",
    "\n",
    "# 최종 데이터 & 타겟 값\n",
    "X_data = np.concatenate((X_data_categorical,X_data_numeric),axis=1)\n",
    "y_data = np.array(data['target']).reshape(-1,1)\n",
    "print(X_data.shape,y_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "# 임베딩을 위한 범주형 데이터 차원\n",
    "categorical_features_size = [len(data[col].unique()) for col in categorical_features]\n",
    "categorical_features_size = [(n,min(100,(n+1)//2)) for n in categorical_features_size]\n",
    "print(categorical_features_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140484, 16]) torch.Size([140484, 1])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , y_train , y_test = train_test_split(X_data, y_data,test_size=0.2, shuffle=True)\n",
    "X_val , X_test , y_val , y_test = train_test_split(X_test, y_test,test_size=0.5, shuffle=True)\n",
    "\n",
    "X_train , y_train = torch.tensor(X_train,dtype=torch.float32) , torch.tensor(y_train,dtype=torch.float32)\n",
    "X_val , y_val = torch.tensor(X_val,dtype=torch.float32) , torch.tensor(y_val,dtype=torch.float32)\n",
    "X_test , y_test = torch.tensor(X_test,dtype=torch.float32) , torch.tensor(y_test,dtype=torch.float32)\n",
    "print(X_train.shape,y_train.shape)\n",
    "\n",
    "# 데이터로더\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_size, layers,layer_size=10, p=0.4,categorical_size=5,numeric_size=11):\n",
    "        super().__init__()\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.categorical_size = categorical_size\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        self.input_batch = nn.BatchNorm1d(numeric_size)\n",
    "\n",
    "        all_layers = []\n",
    "        input_size_t = num_categorical_cols+numeric_size\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size_t, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size_t = i\n",
    "        all_layers.append(nn.Linear(layers[-1], layer_size))\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "        self.outlayer = nn.Linear(layer_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x[:,i].long()))\n",
    "        x1 = torch.cat(embeddings, 1)\n",
    "        x2 = self.input_batch(x[:,self.categorical_size:])\n",
    "        x = torch.cat((x1,x2),1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.outlayer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categorical_features_size,[200,100,50], p=0.2)\n",
    "model.to(device)\n",
    "#print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        mse_loss = nn.MSELoss()\n",
    "        rmse_loss = torch.sqrt(mse_loss(y_pred, y_true))\n",
    "        return rmse_loss\n",
    "RMSEloss = RMSELoss()\n",
    "\n",
    "class MyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        mse_loss = nn.MSELoss(reduction='none')                                            # 각 요소별로 MSE 계산\n",
    "        result = torch.where(y_pred < y_true, torch.tensor(2.0), torch.tensor(1.0))\n",
    "        weighted_loss = result * mse_loss(y_pred, y_true)                                  # 각 요소별로 가중치 적용된 손실 계산\n",
    "        summed_loss = torch.sum(weighted_loss)                                             # 손실 값들을 합산\n",
    "        rmse_loss = torch.sqrt(summed_loss)                                                # RMSE 계산\n",
    "        return rmse_loss\n",
    "myloss = MyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1 train_loss:1617.4173583984375 61%  valid_loss:940.6710205078125 32%\n",
      "Epoch2 train_loss:998.389892578125 34%  valid_loss:951.8718872070312 29%\n",
      "Epoch3 train_loss:982.769287109375 34%  valid_loss:923.7914428710938 32%\n",
      "Epoch4 train_loss:973.6511840820312 34%  valid_loss:925.972412109375 31%\n",
      "Epoch5 train_loss:963.0562744140625 34%  valid_loss:918.4759521484375 33%\n",
      "Epoch6 train_loss:955.578857421875 34%  valid_loss:927.7946166992188 30%\n",
      "Epoch7 train_loss:950.878662109375 34%  valid_loss:925.0840454101562 30%\n",
      "Epoch8 train_loss:945.3604125976562 34%  valid_loss:930.6998901367188 31%\n",
      "Epoch9 train_loss:941.4080810546875 34%  valid_loss:899.0188598632812 33%\n",
      "Epoch10 train_loss:935.7836303710938 34%  valid_loss:911.0087890625 31%\n",
      "Epoch11 train_loss:935.882568359375 34%  valid_loss:915.2188720703125 31%\n",
      "Epoch12 train_loss:930.0257568359375 34%  valid_loss:910.0231323242188 32%\n",
      "Epoch13 train_loss:925.6402587890625 34%  valid_loss:903.5979614257812 32%\n",
      "Epoch14 train_loss:923.5065307617188 34%  valid_loss:883.361328125 35%\n",
      "Epoch15 train_loss:919.4215698242188 34%  valid_loss:905.7083740234375 31%\n",
      "Epoch16 train_loss:917.013671875 34%  valid_loss:903.7352294921875 31%\n",
      "Epoch17 train_loss:914.552734375 34%  valid_loss:883.4622192382812 35%\n",
      "Epoch18 train_loss:913.1382446289062 34%  valid_loss:899.4273681640625 32%\n",
      "Epoch19 train_loss:910.4397583007812 34%  valid_loss:892.6874389648438 33%\n",
      "Epoch20 train_loss:908.0079345703125 34%  valid_loss:878.7554321289062 35%\n",
      "Epoch21 train_loss:904.6162719726562 35%  valid_loss:890.69775390625 34%\n",
      "Epoch22 train_loss:901.9026489257812 34%  valid_loss:917.8528442382812 30%\n",
      "Epoch23 train_loss:901.898681640625 35%  valid_loss:888.4102783203125 33%\n",
      "Epoch24 train_loss:896.21044921875 35%  valid_loss:887.7939453125 36%\n",
      "Epoch25 train_loss:896.382080078125 35%  valid_loss:887.3980712890625 32%\n",
      "Epoch26 train_loss:892.0650634765625 35%  valid_loss:867.4451293945312 38%\n",
      "Epoch27 train_loss:891.318603515625 35%  valid_loss:909.94287109375 30%\n",
      "Epoch28 train_loss:888.5236206054688 35%  valid_loss:893.6409912109375 33%\n",
      "Epoch29 train_loss:889.2361450195312 35%  valid_loss:897.8457641601562 33%\n",
      "Epoch30 train_loss:886.087890625 35%  valid_loss:893.9290771484375 33%\n",
      "Epoch31 train_loss:883.1317138671875 35%  valid_loss:896.325927734375 34%\n",
      "Epoch32 train_loss:882.337890625 35%  valid_loss:896.2901000976562 32%\n",
      "Epoch33 train_loss:882.18505859375 35%  valid_loss:865.6029663085938 38%\n",
      "Epoch34 train_loss:878.15087890625 35%  valid_loss:882.4414672851562 35%\n",
      "Epoch35 train_loss:878.2966918945312 35%  valid_loss:880.1943359375 35%\n",
      "Epoch36 train_loss:877.2798461914062 35%  valid_loss:900.8018188476562 34%\n",
      "Epoch37 train_loss:874.3930053710938 35%  valid_loss:882.8960571289062 35%\n",
      "Epoch38 train_loss:874.9186401367188 35%  valid_loss:887.7694091796875 36%\n",
      "Epoch39 train_loss:872.6936645507812 35%  valid_loss:893.8843383789062 35%\n",
      "Epoch40 train_loss:873.5345458984375 35%  valid_loss:876.1768798828125 37%\n"
     ]
    }
   ],
   "source": [
    "Epoch = 40\n",
    "train_loss , valid_loss = [] , []\n",
    "train_uprate , valid_uprate = [] , []\n",
    "for epoch in range(1,Epoch+1):\n",
    "    total_loss , total_count = 0 , 0\n",
    "    model.train()\n",
    "    for data, y in train_loader:\n",
    "        data , y = data.to(device) , y.to(device)\n",
    "        output = model(data)\n",
    "        rmseloss = RMSEloss(output, y)\n",
    "        uploss = myloss(output,y)\n",
    "        count = torch.sum(output < y).cpu()\n",
    "        optimizer.zero_grad()\n",
    "        uploss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += len(data)*rmseloss.cpu()\n",
    "        total_count += count.item()\n",
    "    epoch_loss = total_loss/len(train_loader.dataset)\n",
    "    train_uprate.append(total_count/len(train_loader.dataset))\n",
    "    train_loss.append(epoch_loss)\n",
    "    print(f'Epoch{epoch} train_loss:{epoch_loss} {100*train_uprate[epoch-1]:.0f}%',end='  ')\n",
    "\n",
    "    total_loss , total_count = 0 , 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, y in valid_loader:\n",
    "            data , y = data.to(device) , y.to(device)\n",
    "            output = model(data)\n",
    "            rmseloss = RMSEloss(output, y)\n",
    "            uploss = myloss(output,y)\n",
    "            count = torch.sum(output < y).cpu()\n",
    "            total_loss += len(data)*rmseloss.cpu()\n",
    "            total_count += count.item()\n",
    "    epoch_loss = total_loss/len(valid_loader.dataset)\n",
    "    valid_loss.append(epoch_loss)\n",
    "    valid_uprate.append(total_count/len(valid_loader.dataset))\n",
    "    print(f'valid_loss:{epoch_loss} {100*valid_uprate[epoch-1]:.0f}%')\n",
    "    torch.save(model.state_dict(), os.path.join('./model_data/', '{0:0=2d}.pth'.format(epoch)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss:887.2420654296875 38%\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "total_loss , total_count = 0 , 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, y in test_loader:\n",
    "        data , y = data.to(device) , y.to(device)\n",
    "        output = model(data)\n",
    "        rmseloss = RMSEloss(output, y)\n",
    "        uploss = myloss(output,y)\n",
    "        count = torch.sum(output < y).cpu()\n",
    "        total_loss += len(data)*rmseloss.cpu()\n",
    "        total_count += count.item()\n",
    "total_loss = total_loss/len(valid_loader.dataset)\n",
    "total_count = total_count / len(test_loader.dataset)\n",
    "print(f'test_loss:{total_loss} {100*total_count:.0f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결론 : RMSE는 887.2 \n",
    "Under-prediction의 비율 : 38%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
